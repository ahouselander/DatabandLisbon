{"cells":[{"cell_type":"markdown","source":["## Overview\n\nThis notebook shows how to use the Databand SDK to observe pipeline execution in Databand. The notebook assumes that the **Spark Listener** has been configured on the cluster that's used to run this notebook. Spark Listener configuration contains URL and credentials for connecting to Databand. It also provides automatic tracking of datasets."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"96816ed7-b08a-4ca3-abb9-f99880c3535d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Install the SDK\n!pip install dbnd-spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1dc3f970-0503-44f2-beb2-1dfc6a6cae16","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Import Databand libraries\nfrom dbnd import dbnd_tracking, task, dataset_op_logger"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8e6d6d65-2b81-4a3b-bc14-f034605d9d34","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Global variables\n\n# databand_url = 'insert_url'\n# databand_access_token = 'insert_token'\n\n# Provide a unique suffix that will be added to various assets tracked in Databand. We use this approach because\n# in a workshop many users are running the same sample pipelines. For example '_mi'\nunique_suffix = '_mi'"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d687e8d-7bab-4030-9e9f-80a6a603d077","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@task\ndef read_raw_data():\n  \n  # Log the data read\n  with dataset_op_logger(\"/FileStore/tables/Retail_Products_and_Customers.csv\", \"read\", with_schema=True, with_preview=True) as logger:\n    retailData = spark.read.csv(\"/FileStore/tables/Retail_Products_and_Customers.csv\", inferSchema=True, header=True, sep=\",\")\n    logger.set(data=retailData)\n  \n  retailData.show()\n    \n  return retailData"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0fc3ac0d-5535-42ba-8eb9-98f0acb0b756","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@task\ndef filter_data(rawData):\n    \n    # Get customers with medium LTV    \n    filteredRetailData = rawData.where(\"LTV = 'MEDIUM VALUE'\")\n    \n    filteredRetailData.show()\n    \n    return filteredRetailData"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bd82bb99-1479-4d5c-be10-8c36df0f1d44","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@task\ndef write_data_by_state(filteredData):\n    \n    from pyspark.sql.functions import col\n\n    oregonSales = filteredData.where(\"State = 'Oregon'\") \n    arizonaSales = filteredData.where(\"State = 'Arizona'\")\n    \n    # Write filtered data\n    with dataset_op_logger(\"dbfs:/retail_data/us/oregon/oregon_sales.csv\", \"write\", with_schema=True, with_preview=True) as logger: \n      oregonSales.write.format(\"csv\").mode(\"overwrite\").save(\"dbfs:/retail_data/us/oregon/oregon_sales.csv\")\n      logger.set(data=oregonSales)\n      \n    with dataset_op_logger(\"dbfs:/retail_data/us/arizona/arizona_sales.csv\", \"write\", with_schema=True, with_preview=True) as logger: \n      oregonSales.write.format(\"csv\").mode(\"overwrite\").save(\"dbfs:/retail_data/us/arizona/arizona_sales.csv\")\n      logger.set(data=arizonaSales)\n             "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b5f66379-6f7f-42ec-8e82-d0e0926a1721","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def prepare_retail_data():\n    \n    with dbnd_tracking(\n            conf={\n                \"core\": {\n                    \"databand_url\": databand_url,\n                    \"databand_access_token\": databand_access_token,\n                }\n            },\n            job_name=\"prepare_sales_data_spark_with_logger\" + unique_suffix,\n            run_name=\"weekly\",\n            project_name=\"Retail Analytics\" + unique_suffix,\n    ):\n        \n        # Call the step job - read data\n        rawData = read_raw_data()\n\n        # Filter data\n        filteredData = filter_data(rawData)\n\n        # Write data by product line\n        write_data_by_state(filteredData)\n\n        print(\"Finished running the pipeline\")\n\n\n# Invoke the main function\nprepare_retail_data()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db9631f6-bb4a-42ca-8a3c-0d48af932331","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Simple_Retail_Pipeline_Spark_Databricks","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":16640}},"nbformat":4,"nbformat_minor":0}
