{"cells":[{"cell_type":"markdown","source":["#Introduction\nDataband provides a few ways to log scripts and datasets in Spark. These options vary in complexity and the amount of code refactoring involved, but each option has its advantages. The below examples will highlight some of the differences as they relate to Databricks notebooks specifically."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a7177911-a12e-4c15-992c-3459d45e2cc5","inputWidgets":{},"title":"Introduction"}}},{"cell_type":"markdown","source":["# Cluster configuration\nYour Databricks cluster will need a few things in order to enable tracking:  \n\n1. Install the Databand SDK by adding it as a PyPI package on your cluster (e.g. `databand[spark]==1.0.11.1`).\n\n2. Specify the following environment variables at a minimum:\n    ```\n    DBND__TRACKING=True\n    DBND__CORE__DATABAND_URL=https://your_env.databand.ai\n    DBND__CORE__DATABAND_ACCESS_TOKEN=your_databand_access_token\n    DBND__ENABLE__SPARK_CONTEXT_ENV=True\n    ```\n\n3. If using Databand's Spark listener, add the JVM agent to your cluster. You can download the JAR and either add it as a library through your cluster's configuration or place it in the folder of your choice, as long as that folder is accessible by the cluster. \n\n4. If using Databand's Spark listener, specify the following in the Spark configuration of your cluster. Be sure to change the path to the listener agent according to where you uploaded it:\n    ```\n    spark.sql.queryExecutionListeners ai.databand.spark.DbndSparkQueryExecutionListener\n    spark.driver.extraJavaOptions -javaagent:/dbfs/FileStore/jars/cf3a7a64_588d_494e_bfc7_13fbb0aa9bb5-dbnd_agent_1_0_11_0_all-1518c.jar\n    ```"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4a729c3-8b86-4576-9313-2e95ec42c36e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Constants\nThe below output path and JSON object will be used for all of the following examples. The file output will be overwritten with each command you run. **Be sure to run the following cell before you continue with the examples below**."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ca509bf5-d556-45ec-8961-c3ad8e11aa00","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["OUTPUT_PATH = \"dbfs:/data/people_output/\"\n\nPEOPLE = [\n    {\n        \"first_name\": \"John\",\n        \"last_name\": \"Smith\",\n        \"age\": 26,\n    },\n    {\n        \"first_name\": \"Harry\",\n        \"last_name\": \"Thompson\",\n        \"age\": 49,\n    },\n    {\n        \"first_name\": \"Sam\",\n        \"last_name\": \"Smith\",\n        \"age\": 36,\n    },\n]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b1db247d-81ce-435b-a441-edf32021224e","inputWidgets":{},"title":"Constants for these examples"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Spark listener\nDataband's Spark listener is integrated at the cluster level. It will log all input/output operations executed on the cluster and send them to Databand as part of the run in which they were executed. When dealing with Databricks notebooks, it is important to note that Databricks does not treat notebook execution the same way that it would the execution of a script submitted to the cluster. For this reason, you will notice that a job tracked through an interactive notebook remains in a running state, even once execution has technically completed. Databand provides a single line of code that can be used as a workaround for this issue.\n\nAlso, it is important to note that Spark does not consider every single dataframe command to be an input or output operation. In the examples below, a manually created JSON object will be used as the basis for a dataframe which will then be filtered and written to storage. In this case, Spark considers the write operation to be an output operation, but it does not consider the manual creation of the dataframe from the JSON object as an input operation. In a production scenario, your inputs will mainly be coming from file or table reads, so this is not a situation you're likely to encounter in the real world."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8809708b-ded6-48c2-93c7-36545f7e25a1","inputWidgets":{},"title":"Spark Listener"}}},{"cell_type":"markdown","source":["### Listener only\nThe below will result in a job in your Databand environment called **Databricks Shell** with a randomly generated run name. This run will remain in a running state since Databricks notebooks do not inherrently close the Spark session once notebook execution has completed. Furthermore, since Databand interprets the run as still being in progress, no datasets that result from I/O operations are transmitted to Databand. \n\nPlease note that this behavior only applies when using Databricks notebooks. This is not an issue when submitting standalone scripts to a Databricks cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"27b33edc-7e1d-4048-a69c-e388dcad2274","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def get_people(json):\n    people_df = spark.read.json(sc.parallelize(json))\n\n    return people_df\n\n\ndef find_all_smiths(df):\n    all_smiths = df[df[\"last_name\"] == \"Smith\"]\n\n    return all_smiths\n\n\ndef save_smiths(df):\n    df.write.mode(\"overwrite\").parquet(OUTPUT_PATH + \"all_smiths.parquet\")\n\n\npeople_df = get_people(PEOPLE)\nall_smiths = find_all_smiths(people_df)\nsave_smiths(all_smiths)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c9e39b9a-6e19-4f81-9905-d7aae4e8a865","inputWidgets":{},"title":"PySpark - Listener only"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Flag the run as completed\nRun the below command once you've validated the tracking behavior for the previous cell. This will result in the open run being marked as complete in Databand, as well as the transmission of any I/O operations picked up by the listener. \n\nFor this run, the **all_smiths** dataset should show up as a write operation within the run. Databand will capture the path of the write, the operation type (read vs. write), the record count, and the schema."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"49e578a9-d48c-48de-bda8-80306173b12e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark._jvm.ai.databand.DbndWrapper.instance().afterPipeline()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d7af1f6-0944-421d-85b4-0b19c0cf2030","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Adding forced run completion\nThe results of executing the following cell should be the same as when you previously ran cells 7 and 9 in sequence, but the tracking context will close immediately without needing to run an extra cell."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef115576-f1f6-495c-bbc6-e15c88abd68e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def get_people(json):\n    people_df = spark.read.json(sc.parallelize(json))\n\n    return people_df\n\n\ndef find_all_smiths(df):\n    all_smiths = df[df[\"last_name\"] == \"Smith\"]\n\n    return all_smiths\n\n\ndef save_smiths(df):\n    df.write.mode(\"overwrite\").parquet(OUTPUT_PATH + \"all_smiths.parquet\")\n\n\npeople_df = get_people(PEOPLE)\nall_smiths = find_all_smiths(people_df)\nsave_smiths(all_smiths)\n\nspark._jvm.ai.databand.DbndWrapper.instance().afterPipeline()  # Include this to signal to Databand that the run has completed. This is only required for notebooks."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"57dbc132-d42d-42ea-916e-d3b3af7a67db","inputWidgets":{},"title":"PySpark - Listener with forced run completion"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Adding custom job and run names\nBy default, the Spark listener will assign any I/O operations resulting from a manually-executed Databricks notebook to a generic pipeline called **Databricks Shell** with a randomly generated run name. If you would like to specify the job and/or run name within Databand, you can do so using environment variables:  \n&nbsp;\n- `DBND__RUN__NAME`: Used to set the pipeline name that will be displayed in Databand\n- `DBND__RUN__JOB_NAME`: Used to set the run name displayed in Databand for an individual execution of your job\n\nAs a best practice, we recommend using a dynamic run name to maintain uniqueness across runs. The simplest way to do this is by appending a timestamp to your run name through your orchestration tool. If this is not an option, you can choose not to set this variable which will continue generating random run names for you in the Databand UI. \n\nThere will not be an example of this included since it would require altering the cluster configuration, but feel free to validate this using your own cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9d370113-e4fd-4c27-8544-198a92cd5354","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Manual SDK integration\nYou may find more value by integrating Databand's SDK functions directly into your code. This will give you much greater control over establishing the tracking context, naming your jobs/runs/projects without changing the cluster configuration, tagging your functions as steps within your job, and logging column-level statistics. The tradeoff is that you will need to refactor your existing code, but you may find this worthwhile given the capabilities this provides. The following examples will showcase some of the features offered by manual SDK integration using the same code as the previous examples for the Spark listener.  \n\n*NOTE*: When not using the listener, you can remove the following parameters from the cluster configuration:  \n&nbsp;  \n- `spark.sql.queryExecutionListeners ai.databand.spark.DbndSparkQueryExecutionListener`\n- `spark.driver.extraJavaOptions -javaagent:/dbfs/path/to/dbnd_agent_X_X_X_X_all.jar`"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f12f5d9d-7948-4c2b-819b-a0a15a53d42c","inputWidgets":{},"title":"Manual SDK Integration"}}},{"cell_type":"markdown","source":["### Creating a tracking context\nThe most vital part of manual SDK integration is establishing a tracking context for your code. When working with notebooks, especially on an ad-hoc basis, you may have certain cells that are used for testing purposes, and you probably do not want those to be tracked within Databand. To control exactly which code gets tracked within your notebook, you can use the `dbnd_tracking` context manager from the Databand SDK. With this function, only the code contained within the tracking context gets sent to Databand. Furthermore, this function allows you to assign job, run, and project names to your code within the Databand UI without needing to change the cluster configuration at all.\n\nNotice in the following examples that you do not need to manually invoke `spark._jvm.ai.databand.DbndWrapper.instance().afterPipeline()` to indicate the completion of your job execution. Since we are specifically tracking the code within our tracking context, Databand knows that tracking has completed once the context is exited. One major advantage of this is that in the event of an error in your code execution, we can immediately report that your job failed in the Databand UI. When using the listener, if your code encounters an error prior to the stop signal being sent, you may end up with a job in your Databand UI that shows that it is running indefinitely.\n\nLastly, it is important to note that any errors tied directly to the Databand SDK will not inhibit the execution of your code. For example, if your Databand environment is down for some reason, and Spark is unable to authenticate with it, your code contained within the tracking context will still execute -- you just won't see that run in the Databand UI."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9eaf648a-d0a0-4bd6-a002-4438eaa386a0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from dbnd import (\n    dbnd_tracking,\n)  # Import the context manager. This should already be available from when you installed the databand[spark] package on your cluster.\n\n\"\"\"\nRecall from cell 12 that it is recommended to always give unique names to your runs. An easy way to do this is by appending a timestamp to your run ID. \n\nAnother option is to simply not specify a run name which will result in Databand assigning a random name to your run automatically. \n\"\"\"\nfrom datetime import (\n    datetime,\n)  # To assist with assigning the current timestamp to your run for uniqueness\n\n\nnow = datetime.now()\nrun_timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef get_people(json):\n    people_df = spark.read.json(sc.parallelize(json))\n\n    return people_df\n\n\ndef find_all_smiths(df):\n    all_smiths = df[df[\"last_name\"] == \"Smith\"]\n\n    return all_smiths\n\n\ndef save_smiths(df):\n    df.write.mode(\"overwrite\").parquet(OUTPUT_PATH + \"all_smiths.parquet\")\n\n\nwith dbnd_tracking(\n    job_name=\"my_databricks_job\",  # The name that will appear on the Pipelines page in your Databand UI\n    run_name=\"my_databricks_job: \"\n    + run_timestamp,  # The name that will appear on the Runs page in your Databand UI\n    project_name=\"my_example_project\",  # The project name assigned to your job. Projects can be used to group related jobs and allow for better filtering in the Databand UI.\n):\n    people_df = get_people(PEOPLE)\n    all_smiths = find_all_smiths(people_df)\n    save_smiths(all_smiths)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a29aa01-14cf-41a4-9e56-42ad6e01bb2d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Assigning task names to functions\nYou will have noticed in the previous examples that the pipeline graph shown on the run page in your Databand UI consists of a single task with the same name as your job. To give your job the look and feel of a typical pipeline, you can use Databand's `@task` decorator to flag the functions in your script as steps in your job.\n\nEach decorated function in your script will be represented by a step in your pipeline graph in the Databand UI. If dependencies are detected between functions (i.e. function_B uses the output of function_A as an input), Databand can draw the appropriate relationship lines between those functions in the graph view.\n\nIn our example, using task decorators should result in a graph that looks similar to the below sequence:  \n&nbsp;  \n[get_people] --> [find_all_smiths] --> [save_smiths]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"be87e70a-7182-4a5f-b26d-743c15408b06","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from dbnd import dbnd_tracking, task  # Import the task decorator\n\nfrom datetime import datetime\n\n\nnow = datetime.now()\nrun_timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\n@task  # Tag each of your functions with the @task decorator so they show up as pipeline steps in the Databand UI\ndef get_people(json):\n    people_df = spark.read.json(sc.parallelize(json))\n\n    return people_df\n\n\n@task\ndef find_all_smiths(df):\n    all_smiths = df[df[\"last_name\"] == \"Smith\"]\n\n    return all_smiths\n\n\n@task\ndef save_smiths(df):\n    df.write.mode(\"overwrite\").parquet(OUTPUT_PATH + \"all_smiths.parquet\")\n\n\nwith dbnd_tracking(\n    job_name=\"my_databricks_job\",\n    run_name=\"my_databricks_job: \" + run_timestamp,\n    project_name=\"my_example_project\",\n):\n    people_df = get_people(PEOPLE)\n    all_smiths = find_all_smiths(people_df)\n    save_smiths(all_smiths)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a2226587-8399-4f42-8b0c-17c294448648","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Logging datasets\nWhen we used Databand's Spark listener in the first set of examples, we were able to capture some high level metadata about the dataset operations happening in our code. The listener is able to automatically log the dataset path, operation type, record count, and schema of any I/O operations that happen on a cluster. In cases where you need more control over *which* datasets get logged, or you want richer metadata such as column-level statistics, you are able to manually track your datasets using Databand's `dataset_op_logger`. \n\nSimilar to `dbnd_tracking`, the `dataset_op_logger` function works best as a context manager for your dataset operations. Below is an example of the usage of `dataset_op_logger` and its parameters:  \n   \n\n```python\nwith dataset_op_logger(\n    \"dbfs://path/to/some/file.csv\", # The full path of your dataset which will become its unique identifier within the Databand UI. This can be a file, table, or URL. \n    \"read\", # Whether the operation is a \"read\" or a \"write\"\n    with_schema=True, # Log the schema of the dataset, default: True\n    with_stats=True, # Log the column-level statistics of the dataset, default: True\n    with_preview=True, # Transmit a preview of the dataset to Databand to view in the UI, default: False\n    with_partition=True, # If the dataset path is partitioned, this will ignore partitioned subfolders (e.g. /column=value/), default: False\n) as logger:\n    people_df = spark.read.json(sc.parallelize(json))\n    logger.set(data=people_df) # Specify the dataset object that should be logged\n```\n\nWhen using `dataset_op_logger`, you should be sure to only include the steps relevant to the dataset operation within the logging context. If an error happens in your code within the logging context, Databand will flag that as a failed dataset operation in the Databand UI. This means that including things unrelated to your dataset operation could potentially create false positives in terms of failed operations. \n\n*NOTE*: When logging column-level statistics, the calculation of statistics is done at runtime, and all column profiling must be completed before your script continues to any subsequent steps. It is very important to consider the tradeoffs between the Spark listener and manual SDK logging when dealing with large (i.e. hundreds of columns, millions of records) datasets:  \n&nbsp;  \n- The Spark listener will add no processing overhead to your Spark script, but you will not have access to column-level statistics.\n- Manual SDK integration gives access to column-level statistics, but calculating those statistics can add varying amounts of overhead to your Spark script.\n\nIn general, we find that most of our customers tend to use the Spark listener due to the fact it that has no performance impact while still providing the ability to generate valuable alerts on your datasets such as schema changes and record count anomalies."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6a102585-57ab-4540-b231-5c1f1beee304","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from dbnd import dbnd_tracking, task, dataset_op_logger  # Import the dataset logger\n\nfrom datetime import datetime\n\n\nnow = datetime.now()\nrun_timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\n@task\ndef get_people(json):\n    \"\"\"\n    In the example below, we provide a fake path to our JSON object since we are manually creating it as part of the notebook. In a real example, this would be the file path, table path, or URL from which your data is being read.\n\n    The only mandatory parameters required for dataset_op_logger are the path and the operation type. We are fine with the defaults for the other parameters in this case.\n\n    Remember from the listener examples that the people_df dataset below did not get logged in Databand. This is because Spark does not consider it to be an I/O operation. When using dataset_op_logger, we have the flexibility to log\n    intermediate datasets that might otherwise not be picked up by the listener due to Spark's lazy evaluation.\n    \"\"\"\n    with dataset_op_logger(\n        \"dbfs:/path/to/people.json\",\n        \"read\",\n    ) as logger:\n        people_df = spark.read.json(sc.parallelize(json))\n        logger.set(data=people_df)\n\n    return people_df\n\n\n@task\ndef find_all_smiths(df):\n    all_smiths = df[df[\"last_name\"] == \"Smith\"]\n\n    return all_smiths\n\n\n@task\ndef save_smiths(df):\n    # The below dataset will retain all of the metadata captured in the listener examples, but it will now display column-level statistics as well.\n    with dataset_op_logger(\n        OUTPUT_PATH + \"all_smiths.parquet\",\n        \"write\",\n    ) as logger:\n        df.write.mode(\"overwrite\").parquet(OUTPUT_PATH + \"all_smiths.parquet\")\n        logger.set(data=df)\n\n\nwith dbnd_tracking(\n    job_name=\"my_databricks_job\",\n    run_name=\"my_databricks_job: \" + run_timestamp,\n    project_name=\"my_example_project\",\n):\n    people_df = get_people(PEOPLE)\n    all_smiths = find_all_smiths(people_df)\n    save_smiths(all_smiths)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8a195217-84de-4961-8409-890d4444de84","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Combining the listener with manual SDK tracking\nDataband's Spark listener and SDK functions are not mutually exclusive. You can combine them if desired to get the best of both worlds. \n\nFor the example below, re-enable the Spark listener if you disabled it for the preceding examples. In this example, we will combine the ease of dataset logging from the Spark listener with the flexibility provided by SDK functions to enable our tracking context and decorate our tasks."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8872addd-eeb4-4261-84f8-9d01a033f82a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from dbnd import dbnd_tracking, task\n\nfrom datetime import datetime\n\n\nnow = datetime.now()\nrun_timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\n@task\ndef get_people(json):\n    # Remember that the people_df dataset does not get picked up by the listener; however, we have the option to manually log it with the SDK if we'd like.\n    people_df = spark.read.json(sc.parallelize(json))\n\n    return people_df\n\n\n@task\ndef find_all_smiths(df):\n    all_smiths = df[df[\"last_name\"] == \"Smith\"]\n\n    return all_smiths\n\n\n@task\ndef save_smiths(df):\n    # Even though we are no longer using dataset_op_logger, this dataset operation will still be logged since the listener is enabled, and it will display it in the Databand UI under the job and run specified below.\n    df.write.mode(\"overwrite\").parquet(OUTPUT_PATH + \"all_smiths.parquet\")\n\n\nwith dbnd_tracking(\n    job_name=\"my_databricks_job\",\n    run_name=\"my_databricks_job: \" + run_timestamp,\n    project_name=\"my_example_project\",\n):\n    people_df = get_people(PEOPLE)\n    all_smiths = find_all_smiths(people_df)\n    save_smiths(all_smiths)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b0b3830d-e27d-4a15-8b86-0b168d5bc59a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Levels of Logging for Databand","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":16608}},"nbformat":4,"nbformat_minor":0}
