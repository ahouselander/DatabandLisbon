{"cells":[{"cell_type":"code","source":["# Install databand - run once\n!pip install databand"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b024696f-7e73-4797-bcd6-0b0a9340ad4c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Import databand libraries\nfrom dbnd import dbnd_tracking, task, dataset_op_logger"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39c75af5-9fde-4483-b810-801f5da2f7bf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Global variables\n\ndataband_url = ' '\ndataband_access_token = ' '\n\n# Data used in this pipeline\nRETAIL_FILE = \"https://raw.githubusercontent.com/elenalowery/data-samples/main/Retail_Products_and_Customers.csv\"\n\n# Provide a unique suffix that will be added to various assets tracked in Databand. We use this approach because\n# in a workshop many users are running the same sample pipelines. For example '_mi'\nunique_suffix = '_el'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26eabb5e-a58f-4d66-a3cb-c16bc7657255"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@task\ndef read_raw_data():\n    \n    import pandas as pd\n    \n    url = 'https://raw.githubusercontent.com/elenalowery/data-samples/main/Retail_Products_and_Customers.csv'\n\n    retailData = pd.read_csv(RETAIL_FILE)\n    \n    # Log the data read\n\n    # Unique name for logging\n    unique_file_name = RETAIL_FILE + unique_suffix\n\n    # Log the data read\n    with dataset_op_logger(unique_file_name,\"read\",with_schema=True,with_preview=True,with_stats=True,with_histograms=True,) as logger:\n        retailData = pd.read_csv(RETAIL_FILE)\n        logger.set(data=retailData)\n    \n    return retailData"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9116b8e7-a0fb-45b0-a00d-fb6b8f0375ce"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@task\ndef filter_data(rawData):\n    \n    unique_file_name = 'script://Weekly_Sales/Filtered_df' + unique_suffix\n\n    # Drop a few columns\n    filteredRetailData = rawData.drop(['Buy', 'PROFESSION', 'EDUCATION'], axis=1)\n\n    with dataset_op_logger(unique_file_name, \"read\", with_schema=True, with_preview=True) as logger:\n        logger.set(data=filteredRetailData)\n    \n    return filteredRetailData"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2ee8859-34cb-4869-8aa5-0f6d51630771"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@task\ndef write_data_by_product_line(filteredData):\n    \n    unique_file_name_1 = 'local://Weekly_Sales/Camping_Equipment.csv' + unique_suffix\n    unique_file_name_2 = 'local://Weekly_Sales/Golf_Equipment.csv' + unique_suffix\n\n    # Select any product line - we will write it to a separate file\n    campingEquipment = filteredData.loc[filteredData['Product line'] == 'Camping Equipment']\n\n    # Log writing the Camping Equipment csv\n    with dataset_op_logger(unique_file_name_1, \"write\", with_schema=True,with_preview=True) as logger:\n        # Write the csv file - in Watson Studio only\n        # project.save_data(\"CampingEquipment.csv\", campingEquipment.to_csv(index=False), overwrite=True)\n        logger.set(data=campingEquipment)\n\n    # Select any product line\n    golfEquipment = filteredData.loc[filteredData['Product line'] == 'Golf Equipment']\n\n    # Log the filtered data read\n    with dataset_op_logger(unique_file_name_2, \"write\", with_schema=True,with_preview=True) as logger:\n        # Write the csv file - in Watson Studio only\n        # project.save_data(\"GolfEquipment.csv\", golfEquipment.to_csv(index=False), overwrite=True)\n        logger.set(data=golfEquipment)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3019dafb-3278-4493-8aa9-4e7add263863"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\n\n# Call and track all steps in a pipeline\n\n# TODO: \n# - Update databand URL and token\n# - Update project and job name (add your initials)\n\ndef prepare_retail_data():\n    \n    with dbnd_tracking(\n            conf={\n                \"core\": {\n                    \"databand_url\": databand_url,\n                    \"databand_access_token\": databand_access_token,\n                }\n            },\n            job_name=\"prepare_sales_data_DB\" + unique_suffix,\n            run_name=\"weekly\",\n            project_name=\"Retail Analytics DB\" + unique_suffix,\n    ):\n        \n        # Call the step job - read data\n        rawData = read_raw_data()\n\n        # Filter data\n        filteredData = filter_data(rawData)\n\n        # Write data by product line\n        write_data_by_product_line(filteredData)\n\n        print(\"Finished running the pipeline\")\n\n\n# Invoke the main function\nprepare_retail_data()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf706cb9-a2a2-4028-83fa-4c87d43dbc13"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Simple_Retail_Demo","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":14086}},"nbformat":4,"nbformat_minor":0}
